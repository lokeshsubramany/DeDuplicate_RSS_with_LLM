{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "    }\n",
    "\n",
    "def get_feed_article_titles_df(feedname,url):\n",
    "    \"\"\"\n",
    "    Get article titles and create a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        feedname (str): Name of the feed.\n",
    "        url (str): URL of the XML feed.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing article titles and feed name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = requests.get(url, headers=headers)\n",
    "\n",
    "        soup = BeautifulSoup(result.text, \"xml\")        \n",
    "        article_urls = [i.text for i in soup.findAll('link')]\n",
    "\n",
    "        #The verge has the links in the id tag, if the list is empty with the link tag, try the id tag\n",
    "        if len([item for item in article_urls if bool(item)])  == 0: \n",
    "            article_urls = [i.text for i in soup.findAll('id')]      \n",
    "        \n",
    "       \n",
    "        #Parse it as html to get the links correctly, other wise In some websites, <media:title> is also returned as a link\n",
    "        soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "        article_titles = [i.text for i in soup.findAll('title')]      \n",
    "        \n",
    "        df = pd.DataFrame({'Article_title': article_titles, 'Article_URL': article_urls[-len(article_titles):], 'Feedname': feedname})\n",
    "        \n",
    "        #Remove homepage from url list and empty url rows\n",
    "        homepage = url.split('.com')[0] + '.com/'\n",
    "        df = df[(df['Article_URL'] != homepage) & (df['Article_URL'] != '') ]        \n",
    "        \n",
    "        # Drop duplicate URLs\n",
    "        df = df.drop_duplicates(subset=['Article_URL'], keep='first')\n",
    "\n",
    "        df['Fetch_Date'] = str(datetime.datetime.now())\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error getting feed: \", e)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        result = requests.get(url[0][0], headers=headers)\n",
    "        soup = BeautifulSoup(result.text, \"html.parser\")   \n",
    "        return (soup.text)     \n",
    "    except Exception as e:        \n",
    "        try:\n",
    "            result = requests.get(url[0], headers=headers)\n",
    "            soup = BeautifulSoup(result.text, \"html.parser\")   \n",
    "            return (soup.text)    \n",
    "        except Exception as e:\n",
    "            try:\n",
    "                result = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(result.text, \"html.parser\")   \n",
    "                return (soup.text)    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_name = 'RssFeeds.db'\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"\n",
    "    Establish a connection to a SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        db_name (str): Name of the SQLite database file.\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3.Connection: Connection object to the SQLite database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con = sqlite3.connect(db_name)\n",
    "        return con\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error connecting to database: \", e)\n",
    "        return None\n",
    "\n",
    "def create_db():\n",
    "    \"\"\"\n",
    "    Create a new SQLite database and execute the given query to create tables.\n",
    "    \n",
    "    Args:\n",
    "        db_name (str): Name of the SQLite database file.\n",
    "        query (str): SQL query to create tables in the database.\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3.Connection: Connection object to the SQLite database.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = [\"CREATE TABLE IF NOT EXISTS FEEDS( Feedname, Article_title UNIQUE, Article_URL, Duplicate, Fetch_Date, Summary)\",\n",
    "    \"CREATE TABLE IF NOT EXISTS SUMMARY( Feedname, Article_URL,Summary)\"]\n",
    "\n",
    "    con = get_connection()\n",
    "    \n",
    "    if con is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        for item in query:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(item)\n",
    "            con.commit()\n",
    "        #return con\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error creating database: \", e)\n",
    "        con.close()\n",
    "        #return None\n",
    "    \n",
    "    print(\"DB created successfully\")\n",
    "\n",
    "def insert_to_db(data, query):\n",
    "    \"\"\"\n",
    "    Insert data into SQLite database using executemany.\n",
    "    \n",
    "    Args:\n",
    "        con (sqlite3.Connection): Connection object to the SQLite database.\n",
    "        data (list of tuples): Data to be inserted into the database.\n",
    "        query (str): SQL query for insertion.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    con = get_connection()\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data to insert.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.executemany(query, data)\n",
    "        con.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error inserting data into database: \", e)\n",
    "        con.rollback()\n",
    "\n",
    "def insert_to_FEEDS(data):\n",
    "    con = get_connection()\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"No data to insert.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "\n",
    "        query = \"INSERT OR REPLACE INTO FEEDS(Article_title,Article_URL,Feedname,Fetch_Date) VALUES (?, ?, ?, ?)\"\n",
    "\n",
    "        cur.executemany(query, data)\n",
    "        con.commit()\n",
    "        con.close()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error inserting data into database: \", e)\n",
    "        con.rollback()\n",
    "        con.close()\n",
    "\n",
    "def insert_to_FEEDS_with_summary(data):\n",
    "    con = get_connection()\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"No data to insert.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "\n",
    "        query = \"INSERT OR REPLACE INTO FEEDS(Feedname, Article_title,Article_URL,Duplicate,Fetch_Date,Summary) VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "\n",
    "        cur.executemany(query, data)\n",
    "        con.commit()\n",
    "        con.close()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error inserting data into database: \", e)\n",
    "        con.rollback()\n",
    "        con.close()\n",
    "\n",
    "def insert_to_Summary(data):\n",
    "    con = get_connection()\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"No data to insert.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "\n",
    "        query = \"INSERT OR REPLACE INTO SUMMARY(Feedname, Article_URL,Summary) VALUES (?, ?, ?)\"\n",
    "\n",
    "        cur.executemany(query, data)\n",
    "        con.commit()\n",
    "        con.close()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error inserting data into database: \", e)\n",
    "        con.rollback()\n",
    "        con.close()\n",
    "\n",
    "\n",
    "def delete_from_db(tablename):\n",
    "    \"\"\"\n",
    "    Delete data from SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        con (sqlite3.Connection): Connection object to the SQLite database.\n",
    "        query (str): SQL query for deletion.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    con = get_connection()\n",
    "    query = \"DROP TABLE IF EXISTS \" + tablename\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(query)\n",
    "        con.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error deleting data from database: \", e)\n",
    "        con.rollback()\n",
    "\n",
    "\n",
    "def query_db(query):\n",
    "    \"\"\"\n",
    "    Execute a SQL query and fetch results from SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        con (sqlite3.Connection): Connection object to the SQLite database.\n",
    "        query (str): SQL query to be executed.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Result set fetched from the database.\n",
    "    \"\"\"\n",
    "    con = get_connection()\n",
    "\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(query)\n",
    "        return cur.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Error executing query: \", e)\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_article_summary(article_text):\n",
    "    # Define the LLM\n",
    "    #llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "    llm = OpenAI(temperature=0, openai_api_key=openai_api_key,model = \"gpt-3.5-turbo-instruct\")\t\t\n",
    "\n",
    "    # Create a chat prompt template\n",
    "    prompt = PromptTemplate.from_template(\"You are an English major with good command of the language. You are able to \\\n",
    "        succintly summarize the meaning behind large bodies of text. Using these skills summarize the text: {article_text}\")\n",
    "    #output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm #| output_parser\n",
    "    try:\n",
    "        result = chain.invoke({\"article_text\": article_text})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedlist={'Engadget':'https://www.engadget.com/rss.xml', \n",
    "          'The Verge':'https://www.theverge.com/rss/index.xml',\n",
    "          'Techcrunch':'https://techcrunch.com/feed/',\n",
    "          'Ars Technica':'https://feeds.arstechnica.com/arstechnica/index',\n",
    "          'Jalopnik':'https://jalopnik.com/rss'}  \n",
    "\n",
    "def refresh_feeds():\n",
    "    \"\"\"\n",
    "    Refresh feeds by retrieving and inserting data for each feed in the feedlist.\n",
    "\n",
    "    This function iterates over each feed in the feedlist, retrieves article data,\n",
    "    and inserts it into the corresponding database table.\n",
    "\n",
    "    Note:\n",
    "    - The feedlist dictionary should contain feed names as keys and their URLs as values.\n",
    "    - The `get_feed_articles_df` function is expected to return a DataFrame with article titles\n",
    "      and URLs for a given feed.\n",
    "    - The `insert_to_FEEDS` function is expected to insert the DataFrame values into the \n",
    "      corresponding database table.\n",
    "\n",
    "    Example usage:\n",
    "    refresh_feeds()\n",
    "\n",
    "    \"\"\"\n",
    "    new_feeds_df = pd.DataFrame()\n",
    "\n",
    "    for feed in feedlist:\n",
    "        print('Getting and inserting data for', feed)\n",
    "        if new_feeds_df.shape[0] == 0:\n",
    "            new_feeds_df = get_feed_article_titles_df(feed, feedlist[feed])        \n",
    "        else:\n",
    "            new_feeds_df = pd.concat([new_feeds_df,get_feed_article_titles_df(feed, feedlist[feed])])\n",
    "\n",
    "\n",
    "    #Get new articles and remove existing articles from the fetched feed list. Logic is below\n",
    "    '''\n",
    "        dfA=pd.DataFrame({'A':[1,2,3],'B':[2,3,4]}) # new data\n",
    "        dfB=pd.DataFrame({'A':[1,6],'B':[2,7]}) # existing data\n",
    "\n",
    "        print(dfA.head())\n",
    "        dfA.set_index('A',inplace=True)\n",
    "        dfB.set_index('A',inplace=True)\n",
    "        newdf=dfA.drop(dfB.index,errors='ignore')\n",
    "    '''\n",
    "    query = \"select Article_title, Article_URL, feedname, Fetch_date from FEEDS where feedname = \" + \"'Jalopnik'\"\n",
    "    existing_feeds_df = pd.read_sql(query,get_connection())\n",
    "    \n",
    "    df = new_feeds_df.set_index('Article_URL').drop(existing_feeds_df['Article_URL'], errors='ignore').reset_index(drop=False)\n",
    "\n",
    "\n",
    "    # for feed in feedlist:\n",
    "    #     print('Getting and inserting data for', feed)\n",
    "    #     df = get_feed_article_titles_df(feed, feedlist[feed])        \n",
    "    #     insert_to_FEEDS(df.values)\n",
    "    insert_to_FEEDS(df.values)   \n",
    "    \n",
    "    print('Added ', len(df), 'new Articles')\n",
    "\n",
    "    df = pd.read_sql(\"Select * from FEEDS\", get_connection())\n",
    "    df.to_csv('feeds.csv', index=False,mode='a')\n",
    "\n",
    "def summarize_feeds_and_store_in_db(n=None):\n",
    "  \n",
    "  df = pd.read_sql(\"Select * from FEEDS where Summary is null\", get_connection())\n",
    "  print('There are ', len(df), 'feeds that need to be summarized')\n",
    "\n",
    "  for i in range(len(df)):    \n",
    "    if i == n:\n",
    "        break\n",
    "\n",
    "    url = df.iloc[i]['Article_URL']\n",
    "    print('Fetching and summarizing ', url)\n",
    "\n",
    "    try:\n",
    "        article_text = get_article_text(url)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching the article text\")\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        summary = get_article_summary(article_text)\n",
    "        df.loc[df['Article_URL'] == url,'Summary'] = summary.strip()  \n",
    "    except Exception as e:\n",
    "        print(\"Error getting article summary\")\n",
    "        print(e)\n",
    "        continue \n",
    "  \n",
    "  df1 = df[df['Summary'].notnull()]\n",
    "\n",
    "  insert_to_FEEDS_with_summary(df1.values)\n",
    "\n",
    "#   df2 = pd.read_sql(\"Select * from FEEDS \", get_connection())\n",
    "#   df2.to_csv('feeds.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB created successfully\n"
     ]
    }
   ],
   "source": [
    "create_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for Engadget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for The Verge\n",
      "Getting and inserting data for Techcrunch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for Ars Technica\n",
      "Getting and inserting data for Jalopnik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  132 new Articles\n",
      "There are  384 feeds that need to be summarized\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022821\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022851\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022820\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022823\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022790\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022794\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022789\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022688\n",
      "Fetching and summarizing  https://www.engadget.com/jack-dorsey-claims-bluesky-is-repeating-all-the-mistakes-he-made-at-twitter-234326121.html?src=rss\n",
      "Fetching and summarizing  https://www.engadget.com/apple-apologizes-for-its-tone-deaf-ad-that-crushed-human-creativity-to-make-an-ipad-211116524.html?src=rss\n"
     ]
    }
   ],
   "source": [
    "refresh_feeds()\n",
    "summarize_feeds_and_store_in_db(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469 entries, 0 to 468\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Feedname       469 non-null    object\n",
      " 1   Article_title  469 non-null    object\n",
      " 2   Article_URL    469 non-null    object\n",
      " 3   Duplicate      1 non-null      object\n",
      " 4   Fetch_Date     469 non-null    object\n",
      " 5   Summary        95 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 22.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"Select * from FEEDS \", get_connection())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  395 feeds that need to be summarized\n",
      "Fetching and summarizing  https://www.theverge.com/2024/5/9/24152926/threads-view-count-feature-now-available\n",
      "Fetching and summarizing  https://www.theverge.com/2024/5/9/24152918/maryland-kids-code-online-privacy-act-netchoice\n",
      "Fetching and summarizing  https://www.theverge.com/24152765/lego-retro-radio-icons\n",
      "Fetching and summarizing  https://www.theverge.com/2024/5/9/24152782/amazon-apple-ipad-deal-sale\n",
      "Fetching and summarizing  https://techcrunch.com/2024/05/09/retell-ai-lets-companies-build-agents-to-answer-their-calls/\n",
      "Fetching and summarizing  https://techcrunch.com/2024/05/09/tiktok-automatically-label-ai-generated-content-created-other-platforms/\n",
      "Fetching and summarizing  https://techcrunch.com/2024/05/09/india-weighs-delaying-caps-on-upi-market-share-in-win-for-phonepe-google-pay/\n",
      "Fetching and summarizing  https://techcrunch.com/2024/05/08/thai-food-delivery-app-line-man-wongnai-weighs-ipo-in-thailand-us-in-2025/\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2023116\n",
      "Fetching and summarizing  https://arstechnica.com/?p=2022871\n"
     ]
    }
   ],
   "source": [
    "summarize_feeds_and_store_in_db(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for Engadget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for The Verge\n",
      "Getting and inserting data for Techcrunch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for Ars Technica\n",
      "Getting and inserting data for Jalopnik\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102 entries, 0 to 101\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Article_URL    102 non-null    object\n",
      " 1   Article_title  102 non-null    object\n",
      " 2   Feedname       102 non-null    object\n",
      " 3   Fetch_Date     102 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "new_feeds_df = pd.DataFrame()\n",
    "\n",
    "for feed in feedlist:\n",
    "    print('Getting and inserting data for', feed)\n",
    "    if new_feeds_df.shape[0] == 0:\n",
    "        new_feeds_df = get_feed_article_titles_df(feed, feedlist[feed])        \n",
    "    else:\n",
    "        new_feeds_df = pd.concat([new_feeds_df,get_feed_article_titles_df(feed, feedlist[feed])])\n",
    "\n",
    "\n",
    "#Get all articles from the DB, and remove existing articles from the fetched feed list. Logic is below\n",
    "'''\n",
    "    dfA=pd.DataFrame({'A':[1,2,3],'B':[2,3,4]}) # new data\n",
    "    dfB=pd.DataFrame({'A':[1,6],'B':[2,7]}) # existing data\n",
    "\n",
    "    print(dfA.head())\n",
    "    dfA.set_index('A',inplace=True)\n",
    "    dfB.set_index('A',inplace=True)\n",
    "    newdf=dfA.drop(dfB.index,errors='ignore')\n",
    "'''\n",
    "query = \"select Article_title, Article_URL, feedname, Fetch_date from FEEDS where feedname = \" + \"'Jalopnik'\"\n",
    "existing_feeds_df = pd.read_sql(query,get_connection())\n",
    "\n",
    "df = new_feeds_df.set_index('Article_URL').drop(existing_feeds_df['Article_URL'], errors='ignore').reset_index(drop=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469 entries, 0 to 468\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Feedname       469 non-null    object\n",
      " 1   Article_title  469 non-null    object\n",
      " 2   Article_URL    469 non-null    object\n",
      " 3   Duplicate      1 non-null      object\n",
      " 4   Fetch_Date     469 non-null    object\n",
      " 5   Summary        74 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 22.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"Select * from FEEDS \", get_connection())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('feeds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  2\n",
      "1  2  3\n",
      "2  3  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([2, 3], dtype='int64', name='A')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA=pd.DataFrame({'A':[1,2,3],'B':[2,3,4]}) # new data\n",
    "dfB=pd.DataFrame({'A':[1,6],'B':[2,7]}) # existing data\n",
    "\n",
    "print(dfA.head())\n",
    "dfA.set_index('A',inplace=True)\n",
    "dfB.set_index('A',inplace=True)\n",
    "newdf=dfA.drop(dfB.index,errors='ignore')\n",
    "\n",
    "newdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B\n",
       "A   \n",
       "1  2\n",
       "2  3\n",
       "3  4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and inserting data for Jalopnik\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51 entries, 1 to 51\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Article_title  51 non-null     object\n",
      " 1   Article_URL    51 non-null     object\n",
      " 2   Feedname       51 non-null     object\n",
      " 3   Fetch_Date     51 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Getting and inserting data for', 'Jalopnik')\n",
    "new_feeds_df = get_feed_article_titles_df('Jalopnik', feedlist['Jalopnik'])        \n",
    "new_feeds_df.info()\n",
    "#insert_to_FEEDS(df.values)\n",
    "\n",
    "# df = pd.read_sql(\"Select * from FEEDS\", get_connection())\n",
    "# df.to_csv('feeds.csv', index=False,mode='a')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75 entries, 0 to 74\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Article_title  75 non-null     object\n",
      " 1   Article_URL    75 non-null     object\n",
      " 2   Feedname       75 non-null     object\n",
      " 3   Fetch_Date     75 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "query = \"select Article_title, Article_URL, feedname, Fetch_date from FEEDS where feedname = \" + \"'Jalopnik'\"\n",
    "existing_feeds_df = pd.read_sql(query,get_connection())\n",
    "existing_feeds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Article_title</th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Fetch_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Article_URL, Article_title, Feedname, Fetch_Date]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDf = new_feeds_df.set_index('Article_URL').drop(existing_feeds_df['Article_URL'], errors='ignore').reset_index(drop=False)\n",
    "newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_title</th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Fetch_Date</th>\n",
       "      <th>Duplicate</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Article_title, Article_URL, Feedname, Fetch_Date, Duplicate, Summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[df3.duplicated(subset=['Article_URL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Article_title</th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Duplicate</th>\n",
       "      <th>Fetch_Date</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Alienware m16 R2 review: When less power makes...</td>\n",
       "      <td>https://www.engadget.com/alienware-m16-r2-revi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:49.748816</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Netflix and Roblox team up for a digital theme...</td>\n",
       "      <td>https://www.engadget.com/netflix-and-roblox-te...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:49.748816</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Nintendo is done paying Elon Musk for X integr...</td>\n",
       "      <td>https://www.engadget.com/nintendo-is-done-payi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:49.748816</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Ember's Travel Mug 2+ with Find My support dro...</td>\n",
       "      <td>https://www.engadget.com/embers-travel-mug-2-w...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:49.748816</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Dungeons and Dragons is coming to Dead by Dayl...</td>\n",
       "      <td>https://www.engadget.com/dungeons-and-dragons-...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:49.748816</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>Who Is The Greatest Racing Driver Of All Time?</td>\n",
       "      <td>https://jalopnik.com/who-is-the-greatest-racin...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:50.932554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>Lighter, More Powerful 2025 BMW M4 CS Is The K...</td>\n",
       "      <td>https://jalopnik.com/lighter-more-powerful-202...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:50.932554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>More Americans Watched The Miami Grand Prix Th...</td>\n",
       "      <td>https://jalopnik.com/more-americans-watched-th...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:50.932554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>You Should At Least Bid On This Special McLare...</td>\n",
       "      <td>https://jalopnik.com/you-should-at-least-bid-o...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:50.932554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>Car Seat Foam Could Expose You To Carcinogens,...</td>\n",
       "      <td>https://jalopnik.com/car-seat-foam-could-expos...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:58:50.932554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feedname                                      Article_title  \\\n",
       "0    Engadget  Alienware m16 R2 review: When less power makes...   \n",
       "1    Engadget  Netflix and Roblox team up for a digital theme...   \n",
       "2    Engadget  Nintendo is done paying Elon Musk for X integr...   \n",
       "3    Engadget  Ember's Travel Mug 2+ with Find My support dro...   \n",
       "4    Engadget  Dungeons and Dragons is coming to Dead by Dayl...   \n",
       "..        ...                                                ...   \n",
       "148  Jalopnik     Who Is The Greatest Racing Driver Of All Time?   \n",
       "149  Jalopnik  Lighter, More Powerful 2025 BMW M4 CS Is The K...   \n",
       "150  Jalopnik  More Americans Watched The Miami Grand Prix Th...   \n",
       "151  Jalopnik  You Should At Least Bid On This Special McLare...   \n",
       "152  Jalopnik  Car Seat Foam Could Expose You To Carcinogens,...   \n",
       "\n",
       "                                           Article_URL Duplicate  \\\n",
       "0    https://www.engadget.com/alienware-m16-r2-revi...      None   \n",
       "1    https://www.engadget.com/netflix-and-roblox-te...      None   \n",
       "2    https://www.engadget.com/nintendo-is-done-payi...      None   \n",
       "3    https://www.engadget.com/embers-travel-mug-2-w...      None   \n",
       "4    https://www.engadget.com/dungeons-and-dragons-...      None   \n",
       "..                                                 ...       ...   \n",
       "148  https://jalopnik.com/who-is-the-greatest-racin...      None   \n",
       "149  https://jalopnik.com/lighter-more-powerful-202...      None   \n",
       "150  https://jalopnik.com/more-americans-watched-th...      None   \n",
       "151  https://jalopnik.com/you-should-at-least-bid-o...      None   \n",
       "152  https://jalopnik.com/car-seat-foam-could-expos...      None   \n",
       "\n",
       "                     Fetch_Date Summary  \n",
       "0    2024-05-09 11:58:49.748816    None  \n",
       "1    2024-05-09 11:58:49.748816    None  \n",
       "2    2024-05-09 11:58:49.748816    None  \n",
       "3    2024-05-09 11:58:49.748816    None  \n",
       "4    2024-05-09 11:58:49.748816    None  \n",
       "..                          ...     ...  \n",
       "148  2024-05-09 11:58:50.932554    None  \n",
       "149  2024-05-09 11:58:50.932554    None  \n",
       "150  2024-05-09 11:58:50.932554    None  \n",
       "151  2024-05-09 11:58:50.932554    None  \n",
       "152  2024-05-09 11:58:50.932554    None  \n",
       "\n",
       "[153 rows x 6 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"Select * from FEEDS where Summary is null\", get_connection())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Article_title</th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Duplicate</th>\n",
       "      <th>Fetch_Date</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Techcrunch</td>\n",
       "      <td>Google DeepMind debuts huge AlphaFold update a...</td>\n",
       "      <td>https://techcrunch.com/2024/05/08/google-deepm...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-08 20:24:46.061625</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Alienware m16 R2 review: When less power makes...</td>\n",
       "      <td>https://www.engadget.com/alienware-m16-r2-revi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:20:34.562828</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Netflix and Roblox team up for a digital theme...</td>\n",
       "      <td>https://www.engadget.com/netflix-and-roblox-te...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:20:34.562828</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Nintendo is done paying Elon Musk for X integr...</td>\n",
       "      <td>https://www.engadget.com/nintendo-is-done-payi...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:20:34.562828</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Ember's Travel Mug 2+ with Find My support dro...</td>\n",
       "      <td>https://www.engadget.com/embers-travel-mug-2-w...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-09 11:20:34.562828</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feedname                                      Article_title  \\\n",
       "0  Techcrunch  Google DeepMind debuts huge AlphaFold update a...   \n",
       "1    Engadget  Alienware m16 R2 review: When less power makes...   \n",
       "2    Engadget  Netflix and Roblox team up for a digital theme...   \n",
       "3    Engadget  Nintendo is done paying Elon Musk for X integr...   \n",
       "4    Engadget  Ember's Travel Mug 2+ with Find My support dro...   \n",
       "\n",
       "                                         Article_URL Duplicate  \\\n",
       "0  https://techcrunch.com/2024/05/08/google-deepm...      None   \n",
       "1  https://www.engadget.com/alienware-m16-r2-revi...      None   \n",
       "2  https://www.engadget.com/netflix-and-roblox-te...      None   \n",
       "3  https://www.engadget.com/nintendo-is-done-payi...      None   \n",
       "4  https://www.engadget.com/embers-travel-mug-2-w...      None   \n",
       "\n",
       "                   Fetch_Date Summary  \n",
       "0  2024-05-08 20:24:46.061625    None  \n",
       "1  2024-05-09 11:20:34.562828    None  \n",
       "2  2024-05-09 11:20:34.562828    None  \n",
       "3  2024-05-09 11:20:34.562828    None  \n",
       "4  2024-05-09 11:20:34.562828    None  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['Summary'].notnull()]\n",
    "\n",
    "insert_to_FEEDS_with_summary(df1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153 entries, 0 to 152\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Feedname       153 non-null    object\n",
      " 1   Article_title  153 non-null    object\n",
      " 2   Article_URL    153 non-null    object\n",
      " 3   Duplicate      0 non-null      object\n",
      " 4   Fetch_Date     153 non-null    object\n",
      " 5   Summary        0 non-null      object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_sql(\"Select * from FEEDS where Summary is null\", get_connection(),)\n",
    "temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Feedname       239 non-null    object\n",
      " 1   Article_title  239 non-null    object\n",
      " 2   Article_URL    239 non-null    object\n",
      " 3   Duplicate      1 non-null      object\n",
      " 4   Fetch_Date     239 non-null    object\n",
      " 5   Summary        86 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 11.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_sql(\"Select * from FEEDS \", get_connection())\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Article_title</th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Duplicate</th>\n",
       "      <th>Fetch_Date</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>OpenAI says it can detect images made by its o...</td>\n",
       "      <td>https://www.engadget.com/openai-says-it-can-de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-08 20:24:45.726655</td>\n",
       "      <td>OpenAI has developed a tool to detect images c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>The M4 iPad Pro is literally lighter than Air</td>\n",
       "      <td>https://www.engadget.com/the-m4-ipad-pro-is-li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-08 20:24:45.726655</td>\n",
       "      <td>The M4 iPad Pro is the latest release from App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>Everything announced at Apple's Let Loose iPad...</td>\n",
       "      <td>https://www.engadget.com/everything-announced-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-08 20:24:45.726655</td>\n",
       "      <td>Advertisement  Advertisement  Advertisement  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>What the heck is going on with Helldivers 2?</td>\n",
       "      <td>https://www.engadget.com/what-the-heck-is-goin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-08 20:24:45.726655</td>\n",
       "      <td>Advertisement  Advertisement  Advertisement  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>TikTok is suing the US government to stop its ...</td>\n",
       "      <td>https://www.engadget.com/tiktok-is-suing-the-u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-08 20:24:45.726655</td>\n",
       "      <td>TikTok is taking legal action against the US g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feedname                                      Article_title  \\\n",
       "0  Engadget  OpenAI says it can detect images made by its o...   \n",
       "1  Engadget      The M4 iPad Pro is literally lighter than Air   \n",
       "2  Engadget  Everything announced at Apple's Let Loose iPad...   \n",
       "3  Engadget       What the heck is going on with Helldivers 2?   \n",
       "4  Engadget  TikTok is suing the US government to stop its ...   \n",
       "\n",
       "                                         Article_URL Duplicate  \\\n",
       "0  https://www.engadget.com/openai-says-it-can-de...       NaN   \n",
       "1  https://www.engadget.com/the-m4-ipad-pro-is-li...       NaN   \n",
       "2  https://www.engadget.com/everything-announced-...       NaN   \n",
       "3  https://www.engadget.com/what-the-heck-is-goin...       NaN   \n",
       "4  https://www.engadget.com/tiktok-is-suing-the-u...       NaN   \n",
       "\n",
       "                   Fetch_Date  \\\n",
       "0  2024-05-08 20:24:45.726655   \n",
       "1  2024-05-08 20:24:45.726655   \n",
       "2  2024-05-08 20:24:45.726655   \n",
       "3  2024-05-08 20:24:45.726655   \n",
       "4  2024-05-08 20:24:45.726655   \n",
       "\n",
       "                                             Summary  \n",
       "0  OpenAI has developed a tool to detect images c...  \n",
       "1  The M4 iPad Pro is the latest release from App...  \n",
       "2  Advertisement  Advertisement  Advertisement  A...  \n",
       "3  Advertisement  Advertisement  Advertisement  A...  \n",
       "4  TikTok is taking legal action against the US g...  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('feeds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_to_FEEDS_with_summary(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86 entries, 0 to 85\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Feedname     86 non-null     object\n",
      " 1   Article_URL  86 non-null     object\n",
      " 2   Summary      86 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"Select Feedname, Article_URL,Summary from FEEDS where Summary is not null\",get_connection())\n",
    "df.info()\n",
    "\n",
    "insert_to_Summary(df[1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedname</th>\n",
       "      <th>Article_URL</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>https://www.engadget.com/openai-says-it-can-de...</td>\n",
       "      <td>OpenAI has developed a tool to detect images c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>https://www.engadget.com/the-m4-ipad-pro-is-li...</td>\n",
       "      <td>The M4 iPad Pro is the latest release from App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>https://www.engadget.com/everything-announced-...</td>\n",
       "      <td>Advertisement  Advertisement  Advertisement  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>https://www.engadget.com/what-the-heck-is-goin...</td>\n",
       "      <td>Advertisement  Advertisement  Advertisement  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>https://www.engadget.com/tiktok-is-suing-the-u...</td>\n",
       "      <td>TikTok is taking legal action against the US g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>https://jalopnik.com/the-best-hybrid-suvs-for-...</td>\n",
       "      <td>The Best Hybrid SUVs For Less Than $45,000 Acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>https://jalopnik.com/the-faa-is-investigating-...</td>\n",
       "      <td>The Federal Aviation Administration is investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Techcrunch</td>\n",
       "      <td>https://techcrunch.com/2024/05/08/google-deepm...</td>\n",
       "      <td>Google DeepMind has released a new version of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>https://www.theverge.com/2024/5/9/24152675/son...</td>\n",
       "      <td>Sonos recently released a controversial redesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>https://www.theverge.com/2024/5/9/24152667/tik...</td>\n",
       "      <td>TikTok is now labeling watermarked third-party...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feedname                                        Article_URL  \\\n",
       "0     Engadget  https://www.engadget.com/openai-says-it-can-de...   \n",
       "1     Engadget  https://www.engadget.com/the-m4-ipad-pro-is-li...   \n",
       "2     Engadget  https://www.engadget.com/everything-announced-...   \n",
       "3     Engadget  https://www.engadget.com/what-the-heck-is-goin...   \n",
       "4     Engadget  https://www.engadget.com/tiktok-is-suing-the-u...   \n",
       "..         ...                                                ...   \n",
       "80    Jalopnik  https://jalopnik.com/the-best-hybrid-suvs-for-...   \n",
       "81    Jalopnik  https://jalopnik.com/the-faa-is-investigating-...   \n",
       "82  Techcrunch  https://techcrunch.com/2024/05/08/google-deepm...   \n",
       "83   The Verge  https://www.theverge.com/2024/5/9/24152675/son...   \n",
       "84   The Verge  https://www.theverge.com/2024/5/9/24152667/tik...   \n",
       "\n",
       "                                              Summary  \n",
       "0   OpenAI has developed a tool to detect images c...  \n",
       "1   The M4 iPad Pro is the latest release from App...  \n",
       "2   Advertisement  Advertisement  Advertisement  A...  \n",
       "3   Advertisement  Advertisement  Advertisement  A...  \n",
       "4   TikTok is taking legal action against the US g...  \n",
       "..                                                ...  \n",
       "80  The Best Hybrid SUVs For Less Than $45,000 Acc...  \n",
       "81  The Federal Aviation Administration is investi...  \n",
       "82  Google DeepMind has released a new version of ...  \n",
       "83  Sonos recently released a controversial redesi...  \n",
       "84  TikTok is now labeling watermarked third-party...  \n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"Select Feedname, Article_URL,Summary from SUMMARY\",get_connection())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
